---
layout: post
title: "Stanford CS231n Lecture 2"
date: 2021-07-18 18:18:28 -0400
categories: [Stanford CS231n]
tags: [DeepLearning, Stanford, CS231n, CNN]
comments: true
math: true
---

### Image classification
- pictures are composed of many **pixels** -> used for classification
- challenges
    - viewpoint variation: pixels are different depending on the angle taken
    - illumination
    - deformation
    - occulsion
    - background cluter
    - intraclass variation
- attempts
    - picture -> find edge -> edge -> find conrners -> recognize<br/>
      -> work well, but this process should be done for all photographs<br/>
      -> can't use
    - Data-Driven Approach
        1. collect data
        2. use ML to train a classifer
        3. evaluate the classifier on new images

        - 1st classifier: **Nearest Neighbor**
            - memorize all data & labels
            - compare with all the pictures and predict the label of the most similar training image
            - train O(1), predict O(N) -> train can be slow, but predict should be fast to use
        - 2nd classifier: **K-Nearest Neigthbor**
            - predict by checking the nearest K data
        - **Distance Metric** to compare images
            - L1(Manhattan) distance<br/>
                <br/>
                $$d_1(I_1, I_2) = \sum_p(I_1^p - I_2^p)$$
                - use when vector pointers are important
                - insensitivity to Outliers 
            - L2(Euclidean) distance<br/>
                <br/>
                $$d_2(I_1,I_2) = \sqrt{\sum_p(I_1^p - I_2^p)^2}$$
                - value does not change when rotating images<br/>
                -> usually used
                - sensitivity to Outliers


### Hyperparameter
: values determined by the user, not by learning<br/>
ex) best value of k, best distance to use => problem-dependent
- setting hyperparameters
    1. choose hyperparameters work best on the data<br/> 
    -> X (only works well with training data)
    2. split data into train and test, choose hyperparameters that work best on test data<br/>
    -> X (does not apply to new data)
    3. split data into train, validation, test: chhose hyperparameters on validation data and evalutate on test data<br/>
    -> better!
    4. croos-validation: split data into folds, try each fold as validation and average the results<br/> 
    -> useful for small datasets, not used frequently in deep learning (when there is a lot of data, the calculation and cost become too large)
    
K-nearset neighbor on images never used
1. very slow at test time
2. distance metrics on pixels are not informative<br/>
-> it says it's the same even if it changes
3. curse of dimensionality<br/>
-> it occurs when the number of variables increases over the data, which means that the space between data increases as dimensions increase

### Linear Classification
image -> $$f(x, W)$$ ($$W$$: weights/parameters) -> giving class scores<br/>
ex) $$f(x,W) = Wx + b$$ (input: $$32\times32\times3=3072$$, $$f:10\times1$$, $$w=10\times3072$$, $$x=3072\times1$$, $$b=10\times1$$)
![1](/images/cs231n/lec2/1.png){: width="100%" height="100%"}
<br/>
<br/>

- hard cases for a linear classifier
    - ![2](/images/cs231n/lec2/2.png){: width="40%" height="40%"} <br/>
    - ![3](/images/cs231n/lec2/3.png){: width="40%" height="40%"} <br/>
    - ![4](/images/cs231n/lec2/4.png){: width="40%" height="40%"} 

<br/>
<br/>
This is written by me after taking CS231n Spring 2017 **provided by Stanford University**.
If you have questions, you can leave a reply on this post.