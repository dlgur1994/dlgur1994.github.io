---
layout: post
title: "Stanford CS231n Lecture14 Reinforcement Learning"
date: 2021-10-10 14:58:28 -0400
categories: [Stanford CS231n]
tags: [DeepLearning, Stanford, CS231n, CNN]
comments: true
math: true
---

### What is Reinforcement Learning?
- ![1](/images/cs231n/lec14/1.png){: width="100%" height="100%"}
- Problems involving an agent interacting with an environment, which provides numeric reward signals (objective, state, action, reward)
- Goal: Learn how to take actions in order to maximize reward
- Keep going until the environment gives back a termianl state
- Example: Cart-Pole problem, Robot Locomotion, Atari Games, Go

### Markov Decision Processes
- Mathematical formulation of the RL problem
- Markov property: No matter what time a specific state is reached or what state has been gone through before, the probability of going to the next state is always the same
- Symbols
    - S: set of possible states
    - A: set of possible actions
    - R: distribution of reward given (state, action) pair
    - P : transition probability i.e. distribution over next state given (state, action) pair
    - $$\gamma$$: discount factor
- Steps
    - At time step t=0, environment samples initial state $$s_0 \rightarrow p(s_0)$$
    - Then, for t=0 until done:
        - Agent selects action $$a_t$$
        - Environment samples reward $$r_t \rightarrow R( . \vert s_t, a_t)$$
        - Environment samples next state $$s_{t+1} \rightarrow P( . \vert s_t, a_t)$$
        - Agent receives reward $$r_t$$ and next state $$s_{t+1}$$
    - A policy $$\pi$$: function from S to A that specifies what action to take in each state
    - Objective: find policy $$\pi^*$$ that maximizes cumulative discounted reward: $$\sum_{t \geq 0} \gamma^t r_t$$
- Example: Grid World
    - ![2](/images/cs231n/lec14/2.png){: width="100%" height="100%"}
    - ![3](/images/cs231n/lec14/3.png){: width="100%" height="100%"}
    - Optimal Policy is more efficient than Random Policy
- Optimal Policy $$\pi^*$$
    - Maximizes the sum of rewards
    - handle the randomness (initial state, transition probability...) $$\rightarrow$$ Maximize the expected sum of rewards
    - ![4](/images/cs231n/lec14/4.png){: width="100%" height="100%"}
    - Value Function
        - at state s, the expected cumulative reward from following the policy from state s
        - ![5](/images/cs231n/lec14/5.png){: width="100%" height="100%"}
    - Q-value function
        - at state s and action a, the expected cumulative reward from taking action a in state s and then following the policy
        - ![6](/images/cs231n/lec14/6.png){: width="100%" height="100%"}
    - Bellman Equation
        - ![7](/images/cs231n/lec14/7.png){: width="100%" height="100%"}
        - the sum of the reward at that point and the expected reward afterwards
        - if the optimal state-action values for the next time-step Q are known, then the optimal strategy is to take the action that maximizes the expected value
        - use Bellman equation as an iterative update
        - Problem: not scalable. Must compute Q(s,a) for every state-action pair. If state is e.g. current game state pixels, computationally infeasible to compute for entire state space
        - Solution: use a function approximator to estimate Q(s,a). E.g. a neural network!

### Q-Learning
- Q-Learning
    - ![8](/images/cs231n/lec14/8.png){: width="100%" height="100%"}
    - Use a function approximator to estimate the action-value function 
    - Deep q-learning: when the function approximator is a deep neural network
- Forward Pass
    - ![9](/images/cs231n/lec14/9.png){: width="100%" height="100%"} 
- Backward Pass
    - ![10](/images/cs231n/lec14/10.png){: width="100%" height="100%"}
- Q-network Architecture
    - ![11](/images/cs231n/lec14/11.png){: width="100%" height="100%"}
    -  Input: state $$s_t$$
    -  Output: 4-d output (if 4 actions), corresponding to $$Q(s_t, a-1), Q(s_t, a_2), Q(s_t, a_3), Q(s_t, a_4)$$
    -  Advantage: a single feedforward pass to compute Q-values for all actions from the current state $$\rightarrow$$ efficient
- Experience Replay
    - Learning from batches of consecutive samples is problematic
        - samples are correlated $$\rightarrow$$ inefficient learning
        - current Q-network parameters determines next training samples $$\rightarrow$$ lead to bad feedback loops
    - Solution: Experience Replay
        - continually update a replay memory table of transitions $$(s_t, a_t, r_t, s_{t+1})$$ as game (experience) episodes are played 
        - train Q-network on random minibatches of transitions from the replay memory, instead of consecutive samples
    - ![11](/images/cs231n/lec14/11.png){: width="100%" height="100%"}
        - initialize replay memory, Q-network
        - play M episodes (full games)
        - initialize state (starting game screen pixels) at the beginning of each episode
        - with small probability, select a random action (explore), otherwise select greedy action from current policy
        - take the action $$(a_t)$$, and observe the reward $$r_t$$ and next state $$s_{t+1}$$
        - store transition in replay memory
        - Experience Replay: sample a random minibatch of transitions from replay memory and perform a gradient descent step

### Policy Gradients

<br/>
<br/>
This is written by me after taking CS231n Spring 2017 **provided by Stanford University**.
If you have questions, you can leave a reply on this post.