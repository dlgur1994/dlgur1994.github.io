---
layout: post
title: "Stanford CS229 Lecture 2"
categories: AI MachineLearning Stanford CS229
date: 2021-04-06 22:16:28 -0400
comments: true
usemathjax: true
---

## 1. Hypothesis
![hypothesis](/images/stanford229/hypothesis.jpg){: width="50%" height="100%"}  
$$h(x) = {\theta}_{0} + {\theta}_{1}x + ... + {\theta}_{n}x_{n}$$ ($$h(x)$$ or $$y$$: output or target, $${\theta}$$: parameter, $$x$$: input or feature)  
$$h(x) = \sum\limits_{j=0}^2{\theta}_{j}x_{j}$$ (where $$x_{0}$$ = 1)    
$$m$$ = the number of training examples (the number of rows in a table)  
$$(x^i,\,y^i)$$: ith training example  
$$n$$ = the number of features  

Choose $${\theta}$$st $$h(x) \approx y$$ for trianing example  
$$h_{\theta}(x)=h(x)$$  
find $${\theta}$$ which miniminzes $$(h_{\theta}(x)-y)^2$$  
$$J({\theta}) = {1 \over z}\sum\limits_{i=1}^m(h_{\theta}(x)-y)^2$$  

## 2. Gradient Descent
$${\theta} \, (say \, {\theta} = \overrightarrow{0})$$  
keep change $${\theta}$$ to reduce $$J({\theta})$$  
$${\theta}_{j} := {\theta}_{j} - \alpha {\partial \over \partial \theta_{j}} J({\theta})$$ ($$j$$: 0,1,2,...n, $$\alpha$$: learning rate)  
$${\partial \over \partial \theta_{j}} J(\theta) = {\partial \over \partial \theta_{j}} {1 \over 2} (h_{\theta}(x) - y )^2$$  
$$= 2 \times {1 \over 2} (h_{\theta}(x) - y) \bullet {\partial \over \partial \theta_{j}} (h_{\theta}(x) - y)$$  
$$= (h_{\theta}(x) - y) \bullet {\partial \over \partial \theta_{j}} (\theta_{0}x_{0} + \theta_{1}x_{1} + \cdots + \theta_{n}x_{n} - y)$$  
$$= (h_{\theta}(x) - y) \bullet x_{j}$$ (when $$j=1$$)  

<!-- $$\theta_{j} := \theta_{j} - \alpha  -->


<br/>
#### * This is written by me after taking CS229 Autumn 2018 provided by Stanford.
#### * If you have questions, you can leave a reply on this post.

