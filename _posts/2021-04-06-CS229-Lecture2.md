---
layout: post
title: "Stanford CS229 Lecture 2"
categories: AI MachineLearning Stanford CS229
date: 2021-04-06 22:16:28 -0400
comments: true
usemathjax: true
---

### 1. hypothesis
![hypothesis](/images/stanford229/hypothesis.jpg){: width="50%" height="100%"}  
$$h(x) = {\theta}_{0} + {\theta}_{1}x + ... + {\theta}_{n}x_{n}$$ ($$h(x)$$ or $$y$$: output or target, $${\theta}$$: parameter, $$x$$: input or feature)  
$$h(x) = \sum\limits_{j=0}^2{\theta}_{j}x_{j}$$ (where $$x_{0}$$ = 1)    
$$m$$ = the number of training examples (the number of rows in a table)  
$$(x^i,\,y^i)$$: ith training example  
$$n$$ = the number of features


- Regression Problem  
![linear regression](/images/stanford229/linear_regression.png){: width="50%" height="50%"}

- Classification Problem  
![classification problems](/images/stanford229/classification_problems.png){: width="50%" height="50%"}

### 2.Unsupervised Learning
- Clustering  
![clustering](/images/stanford229/clustering.jpg){: width="50%" height="100%"}

<br/>
#### * This is written by me after taking CS229 Autumn 2018 provided by Stanford.
#### * If you have questions, you can leave a reply on this post.

