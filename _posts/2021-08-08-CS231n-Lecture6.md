---
layout: post
title: "Stanford CS231n Lecture6 Training Neural Networks, Part 1"
date: 2021-08-08 12:55:28 -0400
categories: [Stanford CS231n]
tags: [DeepLearning, Stanford, CS231n, CNN]
comments: true
math: true
---

### Activation Functions
- Sigmoid
![1](/images/cs231n/lec6/1.png){: width="100%" height="100%"}
    - $$\sigma (x) = 1/(1+e^{-x})$$
    - squashes numbers to range [0,1]
    - historically popular
    - problems
        1. saturated nuerons kill the gradients (saturate: converging to a constant value)
            - at some point, the gradient becomes zero.
        2. outputs are not zero-centered
            - since input is always positive, g gradients on w are always positive or all negative.
            ![2](/images/cs231n/lec6/2.png){: width="100%" height="100%"}
        3. exp() is a bit compute expensive
            - it's less complicated than convolution and dot product, but it can still be a problem.
- tanh(x)
![3](/images/cs231n/lec6/3.png){: width="100%" height="100%"}
    - squashes numbers to range [-1,1]
    - zero centered
    - kills gradients when saturated
    - better than sigmoid
- ReLU (Rectified Linear Unit)
![4](/images/cs231n/lec6/4.png){: width="100%" height="100%"}
    - $$f(x) = max(0,x)$$
    - does not saturate (in + region)
    - computationally efficient
    - convergence of stochastic gradient descent is much faster than sigmoid/tanh in practice
    - more biologically plausible than sigmoid
    - issues
        - not zero-centered output
        - saturatation when $$x \leq 0$$
        ![5](/images/cs231n/lec6/5.png){: width="100%" height="100%"}
            - dead ReLU occurs when there is an error in the initial value setting or the learning rate is too large. If the learning rate is too large and w becomes equal to or less than zero, then it becomes a dead ReLU from that point on.
- Leaky ReLU
![6](/images/cs231n/lec6/6.png){: width="100%" height="100%"}
    - $$f(X) = max(0.01x, x)$$
    - does not saturate
    - computationally efficient
    - converges much faster than sigmoid/tanh in practice
    - **will not die**
- PReLU (Parametric Rectifier)
    - $$f(x) = max(\alpha x, x)$$ 
    - $$\alpha$$: parameter, determined by backprops
    - more flexible
- ELU (Expoential Linear Units)
![7](/images/cs231n/lec6/7.png){: width="100%" height="100%"}
    - ![8](/images/cs231n/lec6/8.png){: width="100%" height="100%"}
    - all benefits of ReLU
    - closer to zero mean outputs
    - negative saturation regime compared with Leaky ReLU adds some robustness to noise
    - compuatation requires exp()
- Maxout Neuron
    - $$max(w_1^Tx + b_1, w_2^Tx + b_2)$$
    - does not have the basic form of dot product -> nonlinearity
    - generalizes ReLU and Leaky ReLU
    - Linear segime, No saturation, Never die
    - doubles the number of parameters/neuron
- Conclusion: **"Use ReLU!"**

## Data Preprocessing
![9](/images/cs231n/lec6/9.png){: width="100%" height="100%"}
- Zero-centering
    - subtract the mean image (e.g. AlexNet)
    - subtraact per-channel mean (e.g. VGGNet)
- Normalization: the image is not in this process because it is already somewhat normalized.
- PCA/Whitening: images do not perform this process because spatial characteristics are used.

## Weight Initialization
- W = 0
    - all neurons do the same operation (occurs if all 'w's are same)
- Small random numbers
    ```python
    W = 0.01 * np.random.randn(D, H)
    ```
    - Works well for small networks, but problems with deeper networks.
- Bigger random numbers
    - almost all neurons completely saturated, either -1 and 1. 
    Gradients will be all zero.
    No updating! 
- Xavier initialization
- 

## Batch Normalization
## Babysitting the Learning Process
## Hyperparameter Optimization


<br/>
<br/>
This is written by me after taking CS231n Spring 2017 **provided by Stanford University**.
If you have questions, you can leave a reply on this post.