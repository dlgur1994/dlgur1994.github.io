---
layout: post
title: "Stanford CS231n Lecture9 CNN Architectures"
date: 2021-08-28 23:37:28 -0400
categories: [Stanford CS231n]
tags: [DeepLearning, Stanford, CS231n, CNN]
comments: true
math: true
---

### LeNet-5
- ![1](/images/cs231n/lec9/1.png){: width="100%" height="100%"}
- early CNN model

### AlexNet
- ![2](/images/cs231n/lec9/2.png){: width="100%" height="100%"}
- input: 227 x 227 x 3 images (in the picture, it says 224, but 227 is correct)
- first layer (CONV 1): 96 11 x 11 filters, stride 4
    - output volume size: (227 - 11) / 4 + 1 = 55 $$\rightarrow$$ 96 x 55 x 55
    - total number of parameters: 11 x 11 x 3 x 96 = 35K
- second layer (POOL1): 3 x 3 filters, stride 2
    - output volume size: (55 - 3) / 2 + 1 = 27 $$\rightarrow$$ 96 x 27 x 27
    - total number of parameters: 0 (parameters are not required because only the largest values are selected in the pooling layer)
- details
    - first use of ReLU
    - used Norm layers (not common anymore)
    - heavy data augmentation
    - dropout 0.5
    - batch size 128
    - SGD Momentum 0.9
    - Learning rate 1e-2, reduced by 10 manually when val accuracy plateaus
    - L2 weight decay 5e-4
    - 7 CNN ensemble

### VGGNet
### GoogLeNet
### ResNet
### Network in Network (NiN)
### Identity Mappings in Deep Residual Networks
### Wide Residual Networks
### Aggregated Residual Transformations for Deep Neural Networks (ResNeXt)
### Deep Networks with Stochastic Depth
### FractalNet: Ultra-Deep Neural Networks without Residuals
### Densely Connected Convolutional Networks
### SqueezeNet: AlexNet-level Accuracy With 50x Fewer Parameters and <0.5Mb Model Size
### Summary.00..0.0

<br/>
<br/>
This is written by me after taking CS231n Spring 2017 **provided by Stanford University**.
If you have questions, you can leave a reply on this post.