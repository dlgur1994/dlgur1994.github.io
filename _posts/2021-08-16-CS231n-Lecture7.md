---
layout: post
title: "Stanford CS231n Lecture7 Training Neural Networks, Part2"
date: 2021-08-16 14:38:28 -0400
categories: [Stanford CS231n]
tags: [DeepLearning, Stanford, CS231n, CNN]
comments: true
math: true
---

### Optimization
- problem with SGD
    - ![1](/images/cs231n/lec7/1.png){: width="100%" height="100%"}
        - the slope is small horizontally and steep vertically
    - ![2](/images/cs231n/lec7/2.png){: width="100%" height="100%"}
        - in practice, saddle points occur more frequently in higher dimensions
        - saddle point: it is a point that is maximum on one axis in a multivariate function, but minimum on some axes.
    - ![3](/images/cs231n/lec7/3.png){: width="100%" height="100%"}
- SGD + Momentum
    - ![4](/images/cs231n/lec7/4.png){: width="100%" height="100%"}
        - because acceleration is considered, it continues even when the speed reaches zero
        - $$\rho$$: hyperparameter to slow down
    - ![5](/images/cs231n/lec7/5.png){: width="100%" height="100%"} 
- Nesterov Momentums
    - ![6](/images/cs231n/lec7/6.png){: width="100%" height="100%"} 
        - SGD + Momentum: Velocity + Gradient -> acutal step
        - Nesterov: Velocity -> Gradient -> actual step (after moving towards velocity, calculate the acutal step by obtaining gradients)
            - $$v_{t+1} = \rho v_t - \alpha \triangledown f(x_t + \rho v_t)$$ (reflect the latest speed as a greater weight)
            - $$x_{t+1} = x_t + v_{t+1}$$
            - $$v_t$$ initialization = 0
            - $$x_t + \rho v_t$$: move towards velocity and get the gradient
            - $$when \ \tilde x_t = x_t + \rho v_t$$, (to make loss function suitable for use)
            - $$v_{t+1} = \rho v_t - \alpha \triangledown f(\tilde x_t)$$
            - $$\tilde x_{t+1} = \tilde x_t - \rho v_t + (1 + \rho) v_{t+1} = \tilde x_t + v_{t+1} + \rho(v_{t+1} - v_t)$$
    - ![7](/images/cs231n/lec7/7.png){: width="100%" height="100%"} 
- AdaGrad
    - ![8](/images/cs231n/lec7/8.png){: width="100%" height="100%"}
    - Q. What happens with AdaGrad?
        - Slow-moving dimensions move fast, fast-moving dimensions move slowly, because dividing by squared gradients makes the rate of updates constant in all dimensions 
    - Q. What happensto the step size over long time?
        - because squared gradients are added, step size is reduced
    - In practice, it's rarely used.
- RMSProp
    - ![9](/images/cs231n/lec7/9.png){: width="100%" height="100%"}
    -  The decay rate prevents the squared gradients from growing rapidly
    -  Decay rate is usually 0.9 or 0.99
- Adam
    - ![10](/images/cs231n/lec7/10.png){: width="100%" height="100%"}
    -  Momentum과 RMSPro를 합친 것이다.
    -  Q. What happens at first timestep when unbiases are missing?
        - Because 'second_moment' is close to zero, x will be a very small number. So at first it moves in a very big step. In some cases, however, this doesn't occur because the 'first_moment' is also offset by being close to zero.
    - In most cases, use Adam. 
- Learning rate in optimizer
    - ![11](/images/cs231n/lec7/11.png){: width="100%" height="100%"}
    -  it was suggested in 'Resnet' that the loss is reduced by reducing the learning rate while circling the epoch
-  First-Order Optimization
    - ![12](/images/cs231n/lec7/12.png){: width="100%" height="100%"}
    - this concept used so far, calculating gradients at a point.
-  Second-Order Optimization
    - ![13](/images/cs231n/lec7/13.png){: width="100%" height="100%"}
    - ![14](/images/cs231n/lec7/14.png){: width="100%" height="100%"}
    - it uses a quadratic approximation to find the value.
    - Q. What is nice about this update?
        - No hyperparameters, No learning rate
    - Q. why is this bad for deep learning?
        - Hessian has $$O(N^2)$$ elemetns. Inverting takes $$O(N^3)$$. N = (Tens or Hundreds of) Millions 
    - Optimization
        - BGFS
        - L-BFGS
            - 
            - usually works very well in full batch, deterministic mode
            - does not transfer very well to mini-batch setting
- Model Ensembles
    - train multiple independent models and average their results at test time $$\rightarrow$$ 2% extra performance
    - instead of training independent models, use multiple snapshots of a single model
    - instead of using actual parameter vector, keep a moving average 

### Regularization
- ways to improve single-model performance
- Adding term to loss
    - ![0](/images/cs231n/lec7/0.png){: width="100%" height="100%"}
    - loss function에 규제값을 붙여준다. 하지만 딥러닝에서는 잘 사용하지 않는다.
- Dropout
    - ![0](/images/cs231n/lec7/0.png){: width="100%" height="100%"} 
    - 신경망을 지나갈 때 임의의 뉴런을 비활성화시키는 것이다.
- Batch Normalization
- Data Augmentation
- DropConnect
- Fractional Max Pooling
- Stochastic Depth
    - 
    - drop some layers

### Transfer Learning
- Using a pre-trained model.
- ![0](/images/cs231n/lec7/0.png){: width="100%" height="100%"}
    - small dataset: transform only the last layer of the pre-trained model, freeze the other layers, and train
    - bigger dataset: train by transforming more layers than in the above way. Because the pre-trained model is well trained, set the learning rate small
- ![0](/images/cs231n/lec7/0.png){: width="100%" height="100%"}
- When using CNN, transfer learning is usually used, rather than training everything from the beginning.

<br/>
<br/>
This is written by me after taking CS231n Spring 2017 **provided by Stanford University**.
If you have questions, you can leave a reply on this post.