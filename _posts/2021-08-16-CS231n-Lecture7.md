---
layout: post
title: "Stanford CS231n Lecture7 Training Neural Networks, Part2"
date: 2021-08-16 14:38:28 -0400
categories: [Stanford CS231n]
tags: [DeepLearning, Stanford, CS231n, CNN]
comments: true
math: true
---

### Optimization
- problem with SGD
    - ![1](/images/cs231n/lec7/1.png){: width="100%" height="100%"}
        - the slope is small horizontally and steep vertically
    - ![2](/images/cs231n/lec7/2.png){: width="100%" height="100%"}
        - in practice, saddle points occur more frequently in higher dimensions
        - saddle point: it is a point that is maximum on one axis in a multivariate function, but minimum on some axes.
    - ![3](/images/cs231n/lec7/3.png){: width="100%" height="100%"}
- SGD + Momentum
    - ![4](/images/cs231n/lec7/4.png){: width="100%" height="100%"}
        - because acceleration is considered, it continues even when the speed reaches zero
        - $$\rho$$: hyperparameter to slow down
    - ![5](/images/cs231n/lec7/5.png){: width="100%" height="100%"} 
- Nesterov Momentums
    - ![6](/images/cs231n/lec7/6.png){: width="100%" height="100%"} 
        - SGD + Momentum: Velocity + Gradient -> acutal step
        - Nesterov: Velocity -> Gradient -> actual step (after moving towards velocity, calculate the acutal step by obtaining gradients)
            - $$v_{t+1} = \rho v_t - \alpha \triangledown f(x_t + \rho v_t)$$ (reflect the latest speed as a greater weight)
            - $$x_{t+1} = x_t + v_{t+1}$$
            - $$v_t$$ initialization = 0
            - $$x_t + \rho v_t$$: move towards velocity and get the gradient
            - $$when \ \tilde x_t = x_t + \rho v_t$$, (to make loss function suitable for use)
            - $$v_{t+1} = \rho v_t - \alpha \triangledown f(\tilde x_t)$$
            - $$\tilde x_{t+1} = \tilde x_t - \rho v_t + (1 + \rho) v_{t+1} = \tilde x_t + v_{t+1} + \rho(v_{t+1} - v_t)$$
    - ![7](/images/cs231n/lec7/7.png){: width="100%" height="100%"} 
- AdaGrad
    - ![8](/images/cs231n/lec7/8.png){: width="100%" height="100%"}
    - Q. What happens with AdaGrad?
        - Slow-moving dimensions move fast, fast-moving dimensions move slowly, because dividing by squared gradients makes the rate of updates constant in all dimensions 
    - Q. What happensto the step size over long time?
        - because squared gradients are added, step size is reduced 
- RMSProp
    -  
- Adam
- Learning rate in optimizer
    -  

### Regularization
- ways to improve single-model performance
- Adding term to loss
    - ![0](/images/cs231n/lec7/0.png){: width="100%" height="100%"}
    - loss function에 규제값을 붙여준다. 하지만 딥러닝에서는 잘 사용하지 않는다.
- Dropout
    - ![0](/images/cs231n/lec7/0.png){: width="100%" height="100%"} 
    - 신경망을 지나갈 때 임의의 뉴런을 비활성화시키는 것이다.
- Batch Normalization
- Data Augmentation
- DropConnect
- Fractional Max Pooling
- Stochastic Depth
    - 
    - drop some layers

### Transfer Learning
- Using a pre-trained model.
- ![0](/images/cs231n/lec7/0.png){: width="100%" height="100%"}
    - small dataset: transform only the last layer of the pre-trained model, freeze the other layers, and train
    - bigger dataset: train by transforming more layers than in the above way. Because the pre-trained model is well trained, set the learning rate small
- ![0](/images/cs231n/lec7/0.png){: width="100%" height="100%"}
- When using CNN, transfer learning is usually used, rather than training everything from the beginning.

<br/>
<br/>
This is written by me after taking CS231n Spring 2017 **provided by Stanford University**.
If you have questions, you can leave a reply on this post.