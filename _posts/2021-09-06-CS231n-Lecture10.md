---
layout: post
title: "Stanford CS231n Lecture10 Recurrent Neural Networks"
date: 2021-09-06 12:16:28 -0400
categories: [Stanford CS231n]
tags: [DeepLearning, Stanford, CS231n, CNN]
comments: true
math: true
---

### Vanlia Neural Network vs Recurrent Neural Networks
- ![1](/images/cs231n/lec10/1.png){: width="100%" height="100%"}
- ![2](/images/cs231n/lec10/2.png){: width="100%" height="100%"}
    - e.g. 
        - one to many: image captioning
        - many to one: sentiment calssification
        - many to many: machine translation, video classification on frame level

### Recurrent Neural Network
- ![3](/images/cs231n/lec10/3.png){: width="100%" height="100%"}
- ![4](/images/cs231n/lec10/4.png){: width="100%" height="100%"}
- the same function and the same set of parameters are used at every time step
- Vanila Recurrent Neural Network
    - ![5](/images/cs231n/lec10/5.png){: width="100%" height="100%"}
    - $$W_{hh}$$: weight from hidden state
    - $$W_{xh}$$: weight from x
    - $$W_{hy}$$: weight to y
- ![6](/images/cs231n/lec10/6.png){: width="100%" height="100%"}
- ![7](/images/cs231n/lec10/7.png){: width="100%" height="100%"}
    - $$h_T$$: final hidden state $$\rightarrow$$ summarizes all of the context
- ![8](/images/cs231n/lec10/8.png){: width="100%" height="100%"}
    - a fixed input initializes the initial hidden state of the model 
- ![9](/images/cs231n/lec10/9.png){: width="100%" height="100%"}
    - encoder: take variable inputs and summarize the whole thing at the end
    - decoder: make a variable output from the summarized data
- Example: Characeter-level Language Model
    -  ![10](/images/cs231n/lec10/10.png){: width="100%" height="100%"}
        - vocabulary: [h, e, l, o]
        - training sequence: "hello"
        - values of output layers are not close to correct values, but after training by calculating loss values, the values of the correct characters increase
    - ![11](/images/cs231n/lec10/11.png){: width="100%" height="100%"}
        - when test-time was given an input of "h", "e" was selected
        - "e" becomes the input of the next step and passes through the model, creating a new output and continuing
        - for the first time, the softamx value is "o" the largest, but luckily "e" is selected.
        - for this problem, test-time does not select the alphabet with the highest softmax value, but sampling (random selection of one by probability)
        - in general, sampling lets get diversity from the model
- Truncated Backpropagation through time
    - ![12](/images/cs231n/lec10/12.png){: width="100%" height="100%"}
        - if the data is large, there is a problem of slowing down
    - ![13](/images/cs231n/lec10/13.png){: width="100%" height="100%"}
        - run forward and backward through chunks of the sequence instead of whole sequence 
        - carry hidden states forward in time forever, but only backpropagate for some smaller number of steps

### Image Captioning
- ![14](/images/cs231n/lec10/14.png){: width="100%" height="100%"}
    - a vector containing information in the image is obtained through CNN, and a sentence is generated through RNN
- ![15](/images/cs231n/lec10/15.png){: width="100%" height="100%"}
    - the softmax at the end of the CNN model is eliminated, and a vector containing the entire degree of the image is generated through the fc-4096 layer
- ![16](/images/cs231n/lec10/16.png){: width="100%" height="100%"}
    - needs start token to start
    - calculated by sending vector information obtained from CNN to h0
    - one output is the input of the next step
    - RNN ends when end token is created (e.g. y2)

### Attention
- Image Captioning
    - ![17](/images/cs231n/lec10/17.png){: width="100%" height="100%"}
        - RNN focuses its attention at a different spatial location when generating each word
    - ![18](/images/cs231n/lec10/18.png){: width="100%" height="100%"}
        - h0: create a distribution of the location to concentrate in the image (a1)
        - a1: computes the original vector (L $$\times$$ D) to create an attention (z1)
        - z1 and the first word(y1) enter h1 and a2 and distribution over vocabularay words (d1) are created
    - results
        - ![19](/images/cs231n/lec10/19.png){: width="100%" height="100%"}
            - soft attention: take a weighted combination of all features from all image locations
            - hard attention: select exactly one location to look at in the image at each time step
        - ![20](/images/cs231n/lec10/20.png){: width="100%" height="100%"}
- Visual Question Answering
    - ![21](/images/cs231n/lec10/21.png){: width="100%" height="100%"}
    - ![22](/images/cs231n/lec10/22.png){: width="100%" height="100%"}
        - get a summary of the image via CNN and a summary of the input question via RNN 
        - predict the correct answer through the obtained summaries

### LSTM
- ![23](/images/cs231n/lec10/23.png){: width="100%" height="100%"}
- ![24](/images/cs231n/lec10/24.png){: width="100%" height="100%"}
    - problems arise as the weights continue to multiply
    - when weights are large, it can be solved by gradient clipping, which is using normalizing, but when they are small, there is only a way to change RNN architecture
- ![25](/images/cs231n/lec10/25.png){: width="100%" height="100%"} 
- ![26](/images/cs231n/lec10/26.png){: width="100%" height="100%"} 
- ![27](/images/cs231n/lec10/27.png){: width="100%" height="100%"}
    - Backpropagation from $$c_t$$ to $$c_{t-1}$$ only elementwise c multiplication by f, no matrix t multiply by W 


<br/>
<br/>
This is written by me after taking CS231n Spring 2017 **provided by Stanford University**.
If you have questions, you can leave a reply on this post.