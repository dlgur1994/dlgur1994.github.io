<!-- ---
layout: post
title: "Stanford CS229 Lecture 2"
categories: AI MachineLearning Stanford CS229
date: 2021-04-22 20:04:28 -0400
comments: true
usemathjax: true
---

## 1. Hypothesis
![hypothesis](/images/stanford229/hypothesis.jpg){: width="50%" height="100%"}  
$$h(x) = {\theta}_{0} + {\theta}_{1}x + ... + {\theta}_{n}x_{n}$$ ($$h(x)$$ or $$y$$: output or target, $${\theta}$$: parameter, $$x$$: input or feature)  
$$h(x) = \sum\limits_{j=0}^2{\theta}_{j}x_{j}$$ (where $$x_{0}$$ = 1)    
$$m$$ = the number of training examples (the number of rows in a table)  
$$(x^i,\,y^i)$$: ith training example  
$$n$$ = the number of features  

Choose $${\theta}$$st $$h(x) \approx y$$ for trianing example  
$$h_{\theta}(x)=h(x)$$  
find $${\theta}$$ which miniminzes $$(h_{\theta}(x)-y)^2$$  
$$J({\theta}) = {1 \over z}\sum\limits_{i=1}^m(h_{\theta}(x)-y)^2$$  
<br/>

## 2. Gradient Descent
$${\theta} \, (say \, {\theta} = \overrightarrow{0})$$  
keep change $${\theta}$$ to reduce $$J({\theta})$$  
$${\theta}_{j} := {\theta}_{j} - \alpha {\partial \over \partial \theta_{j}} J({\theta})$$ ($$j$$: 0,1,2,...n, $$\alpha$$: learning rate)  
$${\partial \over \partial \theta_{j}} J(\theta) = {\partial \over \partial \theta_{j}} {1 \over 2} (h_{\theta}(x) - y )^2$$  
$$= 2 \times {1 \over 2} (h_{\theta}(x) - y) \cdot {\partial \over \partial \theta_{j}} (h_{\theta}(x) - y)$$  
$$= (h_{\theta}(x) - y) \cdot {\partial \over \partial \theta_{j}} (\theta_{0}x_{0} + \theta_{1}x_{1} + \cdots + \theta_{n}x_{n} - y)$$  
$$= (h_{\theta}(x) - y) \cdot x_{j}$$ (when $$j=1$$)  
<br/>

## 3. Batch Gradient Descent
: It proceeds in a way that reduces $$J(\theta)$$, and if there is a lot of data, it is slow  
$${\partial \over \partial \theta_{j}} J(\theta) = \sum \limits_{i=1}^m (h_{\theta}(x^{(i)}-y^{(i)}) x_{j}^{(i)}$$  
$$\theta_{j} := \theta_{j} - \alpha \sum \limits_{i=1}^m (h_{\theta}(x^{(i)}-y^{(i)}) x_{j}^{(i)}$$ (for $$j = 0,1, \cdots , n$$)  
<br/>

## 4. Stochastic Gradient Descent
: It may not be the best, but it is similar to the best, and if there is a lot of data, it is fast.  
Repeat {  
$$\quad$$ for $$i = 1$$ to $$M$$ {  
$$\qquad$$ $$\theta_{j} := \theta_{j} - \alpha (h_{\theta}(x^{(i)}) - y^{(i)}) \cdot x_{j}^{(i)}$$  
$$\quad$$ }  
}  
<br/>

## 5. Normal Equation
<!-- : it works for only linear regression    
$$ \bigtriangledown_{\theta} J(\theta) = \left[ \begin{matrix} {\partial J} \over {\partial \theta_{0}} \\ {\partial J} \over {\partial \theta_{1}} \\ \cdots \\ {\partial J} \over {\partial \theta_{n}} \\ \end{matrix} \right] (\theta \in \mathbb{R} ^{n+1}, \, n: # of features)$$ -->

<!-- ex) $$A \in \mathbb{R}^{2 \times 2}$$, $$A = \left[ \begin{matrix} A_{11} & A_{12} \\ A_{21} & A_{22} \\ \end{matrix} \right]$$, $$f: \mathbb{R}^{2 \times 2} \rightarrow \mathbb{R}$$  
$$f(A) = A_{11} + A_{12}^2$$,   $$f(\left[ \begin{matrix} 5 & 6 \\ 7 & 8 \\ \end{matrix} \right]) = 5 + 6^2$$   -->
<!-- $$\bigtriangledown_{\theta} f(A) = \left[ \begin{matrix} {{\partial f} \over {\partial A_{11}}} & {{\partial f} \over {\partial A_{12}}} \\ {{\partial f} \over {\partial A_{21}}} & {{\partial f} \over {\partial A_{22}}} \\ \end{matrix} \right]$$   -->
<!-- $$ \bigtriangledown_{A} f(A) = \left[ \begin{matrix} 1 & 2A_{12} \\ 0 & 0 \\ \end{matrix} \right]$$  
<br>

&#42; If A -> square matrix $$(A \in \mathbb{R}^2)$$  
$$tr A =$$ sum of diagonal entries  
= $$\sum_{i} A_{ii} = $$ trace of A $$= tr(A)$$  

&#42; $$tr(A) = tr(A^T)$$  

&#42; if f(A) = tr(AB) (B: fixed matrix), $$\bigtriangledown_{A} f(A) = B^T$$  

&#42; $$tr(AB) = tr(BA)$$  

&#42; $$tr(ABC) = tr(CAB)$$  

&#42; \bigtriangle_{A} tr(AA^TC) = CA + C^TA ({\partial \over \partial a} a^2 c = 2ac)$$  

&#42; $$J(\theta) = {1 \over 2} \sum_{i=1}^m (h(x^i)-y^i)^2 = {1 \over 2}(X\theta - y)^T(X\theta-y)$$  
$$x_{\theta} = \left[ \begin{matrix} __(x^1)^T__ \\ __(x^2)^T__ \\ \vdots \\ __(x^m)^T__ \end{matrix} \right] \left[ \begin{matrix} \theta \\ \theta \\ \cdots \\ \theta \end{matrix} \right]$$  
$$= \left[ \begin{matrix} (x^1)^T \theta \\ (x^2)^T \theta \\ \vdots \\ (x^m)^T \theta \end{matrix} \right] = \left[ \begin{matrix} h \theta x^1 \\ h \theta x^2 \\ \vdots \\ h \theta x^m \end{matrix} \right]$$  
$$y = \left[ \begin{matrix} y^1 \\ y^2 \\ vdots \\ y^m \end{matrix} \right]$$  
$$x \theta - y = \left[ \begin{matrix} h_{\theta}(x^1) - y^1 \\ \vdots \\ h_{\theta}(x^m) - y^m \end{matrix} \right] \, (z^Tz = \sum_{i}Z^2)$$  

&#42; $$\bigtriangle_{\theta} J(\theta) = \bigtriangle_{\theta} {1 \over 2} (X \theta - y)^T (X \theta - y)$$   
$$= {1 \over 2} \bigtriangle_{\theta} (\theta^T X^T - y^T)(X \theta - y)$$  
$$= {1 \voer 2} \bigtriangle_{\theta} (\theta^T X^T X \theta - \theta^T X^T y - y^T X \theta + y^T y)$$  
$$= {1 \over 2} (X^T X \theta + X^T X \theta - X^T y - X^T y)$$  
$$= X^T X \theta - X^T y = \vec{0}$$  
$$X^X \theta = X^Ty$$ ("normal equation")  
$$\theta = (X^TX)^-1 X^T y$$  
<br/>

#### * This is written by me after taking CS229 Autumn 2018 provided by Stanford.
#### * If you have questions, you can leave a reply on this post.
 -->
